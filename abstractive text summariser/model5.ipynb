{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_216580\\1500256061.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "from tensorflow.python.keras.layers import Layer\n",
    "\n",
    "from utils.attention import AttentionLayer\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = AttentionLayer(name = 'attention_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"D:\\Datasets\\news_summary_more.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(text):\n",
    "#     text = text.lower()  # lowering the text\n",
    "#     text = [contractions.fix(word) for word in text.split(\" \")]  # word contraction\n",
    "#     text = \" \".join(text)\n",
    "#     text = [word for word in text.split(\" \") if word not in stopwords.words('english')]\n",
    "#     text = \" \".join(text)\n",
    "#     text = re.sub(r\"\\'s \",\"\",text)   # remove 's from the sentence\n",
    "#     text =  re.sub(r\"\\(.*\\)\",\"\",text) # remove words written in ()\n",
    "#     text = re.sub(r'[^a-zA-Z0-9. ]','',text)\n",
    "#     text = re.sub(r'\\.','. ',text)\n",
    "#     text = re.sub(r'\\s+',' ',text)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# applying preprocessing function on text and headlines\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadlines\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheadlines\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m [contractions\u001b[38;5;241m.\u001b[39mfix(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)]  \u001b[38;5;66;03m# word contraction\u001b[39;00m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[1;32m----> 5\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,text)   \u001b[38;5;66;03m# remove 's from the sentence\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m [contractions\u001b[38;5;241m.\u001b[39mfix(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)]  \u001b[38;5;66;03m# word contraction\u001b[39;00m\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[1;32m----> 5\u001b[0m text \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,text)   \u001b[38;5;66;03m# remove 's from the sentence\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# applying preprocessing function on text and headlines\n",
    "\n",
    "# dataset['headlines'] = dataset['headlines'].apply(preprocess)\n",
    "# dataset['text'] = dataset['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv(\"dataset_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new = pd.read_csv(\"dataset_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>upgrad learner switches career ml al 90 salary...</td>\n",
       "      <td>saurav kant alumnus upgrad iiitbpg program mac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>delhi techie wins free food swiggy one year cred</td>\n",
       "      <td>kunal shahcredit card bill payment platform cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>new zealand end rohit sharmaled india12match w...</td>\n",
       "      <td>new zealand defeated india 8 wickets fourth od...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>known hirani yrs metoo claims true sonam</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          headlines  \\\n",
       "0           0  upgrad learner switches career ml al 90 salary...   \n",
       "1           1   delhi techie wins free food swiggy one year cred   \n",
       "2           2  new zealand end rohit sharmaled india12match w...   \n",
       "3           3  aegon life iterm insurance plan helps customer...   \n",
       "4           4           known hirani yrs metoo claims true sonam   \n",
       "\n",
       "                                                text  \n",
       "0  saurav kant alumnus upgrad iiitbpg program mac...  \n",
       "1  kunal shahcredit card bill payment platform cr...  \n",
       "2  new zealand defeated india 8 wickets fourth od...  \n",
       "3  aegon life iterm insurance plan customers enjo...  \n",
       "4  speaking sexual harassment allegations rajkuma...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_new['headlines'] = dataset_new[\"headlines\"].apply(lambda x: \"_start_ \" + x +\" _end_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = []\n",
    "output_lengths = []\n",
    "\n",
    "for i,j in zip(dataset_new[\"headlines\"],dataset_new['text']):\n",
    "    input_lengths.append(len(j.split()))\n",
    "    output_lengths.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = {}\n",
    "with open(r\"D:\\Datasets\\glove\\glove.42B.300d.txt\\glove.42B.300d.txt\",encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        values =  line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:],'float32')\n",
    "        embedding_matrix[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words present 161005\n",
      "total interleaving words present are 46.0 %\n"
     ]
    }
   ],
   "source": [
    "# counting unique words from input text since the output is the summary the words would be present in it.\n",
    "\n",
    "word_corpus = set()\n",
    "\n",
    "for line in dataset_new['text']:\n",
    "    for word in line.split():\n",
    "        if word not in word_corpus:\n",
    "            word_corpus.add(word)\n",
    "            \n",
    "unique_words = len(word_corpus)\n",
    "\n",
    "# words present in unique words and corpus\n",
    "\n",
    "# inter_words = []\n",
    "\n",
    "# for word in embedding_matrix.keys():\n",
    "#     if word in word_corpus:\n",
    "#         inter_words.append(word)\n",
    "\n",
    "inter_words = set(embedding_matrix.keys()).intersection(word_corpus)\n",
    "\n",
    "print(\"total words present {}\".format(len(word_corpus)))\n",
    "print('total interleaving words present are {} %'.format(np.round((float(len(inter_words))/len(word_corpus))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the words in the corpus 3618225\n",
      "the unique words in the corpus 161006\n",
      "The number of words that are present in both glove vectors and our corpus are 73849 which is nearly 46.0% \n"
     ]
    }
   ],
   "source": [
    "words_source_train = []\n",
    "for i in dataset_new['text'] :\n",
    "  words_source_train.extend(i.split(' '))\n",
    "\n",
    "print(\"all the words in the corpus\", len(words_source_train))\n",
    "words_source_train = set(words_source_train)\n",
    "print(\"the unique words in the corpus\", len(words_source_train))\n",
    "inter_words = set(embedding_matrix.keys()).intersection(words_source_train)\n",
    "print(\"The number of words that are present in both glove vectors and our corpus are {} which \\\n",
    "is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))/len(words_source_train))\n",
    "*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73849"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_source_corpus = {}\n",
    "\n",
    "glove = set(embedding_matrix.keys())\n",
    "\n",
    "for word in word_corpus:\n",
    "    if word in glove:\n",
    "        word_source_corpus[word] = embedding_matrix[word]\n",
    "        \n",
    "len(word_source_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num(text):\n",
    "    unique_w = [w for w in text.split() if w not in inter_words]\n",
    "    return len(unique_w)\n",
    "\n",
    "dataset_new['unique_words'] = dataset_new['text'].apply(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>_start_ upgrad learner switches career ml al 9...</td>\n",
       "      <td>saurav kant alumnus upgrad iiitbpg program mac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>_start_ delhi techie wins free food swiggy one...</td>\n",
       "      <td>kunal shahcredit card bill payment platform cr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>_start_ new zealand end rohit sharmaled india1...</td>\n",
       "      <td>new zealand defeated india 8 wickets fourth od...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>_start_ aegon life iterm insurance plan helps ...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>_start_ known hirani yrs metoo claims true son...</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          headlines  \\\n",
       "0           0  _start_ upgrad learner switches career ml al 9...   \n",
       "1           1  _start_ delhi techie wins free food swiggy one...   \n",
       "2           2  _start_ new zealand end rohit sharmaled india1...   \n",
       "3           3  _start_ aegon life iterm insurance plan helps ...   \n",
       "4           4  _start_ known hirani yrs metoo claims true son...   \n",
       "\n",
       "                                                text  unique_words  \n",
       "0  saurav kant alumnus upgrad iiitbpg program mac...             5  \n",
       "1  kunal shahcredit card bill payment platform cr...             4  \n",
       "2  new zealand defeated india 8 wickets fourth od...             3  \n",
       "3  aegon life iterm insurance plan customers enjo...             0  \n",
       "4  speaking sexual harassment allegations rajkuma...             2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>_start_ new zealand end rohit sharmaled india1...</td>\n",
       "      <td>new zealand defeated india 8 wickets fourth od...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>_start_ aegon life iterm insurance plan helps ...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>_start_ known hirani yrs metoo claims true son...</td>\n",
       "      <td>speaking sexual harassment allegations rajkuma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>_start_ rahat fateh ali khan denies getting no...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>_start_ govt directs alok verma join work 1 da...</td>\n",
       "      <td>weeks excbi director alok verma told departmen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          headlines  \\\n",
       "0           2  _start_ new zealand end rohit sharmaled india1...   \n",
       "1           3  _start_ aegon life iterm insurance plan helps ...   \n",
       "2           4  _start_ known hirani yrs metoo claims true son...   \n",
       "3           5  _start_ rahat fateh ali khan denies getting no...   \n",
       "4           7  _start_ govt directs alok verma join work 1 da...   \n",
       "\n",
       "                                                text  unique_words  \n",
       "0  new zealand defeated india 8 wickets fourth od...             3  \n",
       "1  aegon life iterm insurance plan customers enjo...             0  \n",
       "2  speaking sexual harassment allegations rajkuma...             2  \n",
       "3  pakistani singer rahat fateh ali khan denied r...             2  \n",
       "4  weeks excbi director alok verma told departmen...             1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_new = dataset_new[dataset_new['unique_words'] < 4]\n",
    "\n",
    "dataset_new.reset_index(inplace=True,drop=True)\n",
    "\n",
    "dataset_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val = train_test_split(dataset_new['text'],dataset_new['headlines'],test_size=5,random_state=0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_val, y_val, test_size = 0.5, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "max_text_len = max(input_lengths)\n",
    "max_headline_len = max(output_lengths)\n",
    "\n",
    "print(max_text_len)\n",
    "print(max_headline_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = Tokenizer()\n",
    "\n",
    "x_t.fit_on_texts(dataset_new['text'] + dataset_new['headlines'])\n",
    "\n",
    "x_vocab_size = len(x_t.word_index) + 1\n",
    "\n",
    "encoded_x_train = x_t.texts_to_sequences(X_train)\n",
    "encoded_x_val = x_t.texts_to_sequences(X_val)\n",
    "encoded_x_test = x_t.texts_to_sequences(X_test)\n",
    "\n",
    "padded_X_train = pad_sequences(encoded_x_train, maxlen=max_text_len,padding='post')\n",
    "paddad_X_val = pad_sequences(encoded_x_val,maxlen=max_text_len,padding='post')\n",
    "padded_X_test = pad_sequences(encoded_x_test,maxlen=max_text_len,padding='post')\n",
    "\n",
    "\n",
    "\n",
    "y_t = Tokenizer()\n",
    "y_t.fit_on_texts(dataset_new['headlines'])\n",
    "y_vocab_size = len(y_t.word_index) + 1\n",
    "\n",
    "encoded_y_train = y_t.texts_to_sequences(y_train)\n",
    "encoded_y_val = y_t.texts_to_sequences(y_val)\n",
    "encoded_y_test = y_t.texts_to_sequences(y_test)\n",
    "\n",
    "paddad_y_train = pad_sequences(encoded_y_train,maxlen=max_headline_len,padding='post')\n",
    "paddad_y_test = pad_sequences(encoded_y_test,maxlen=max_headline_len,padding='post')\n",
    "padded_y_val = pad_sequences(encoded_y_val,maxlen=max_headline_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix_n = np.zeros((x_vocab_size,embedding_dim),dtype=np.float32)\n",
    "\n",
    "for word,i in x_t.word_index.items():\n",
    "    embedding_vector = embedding_matrix.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix_m = np.zeros((x_vocab_size,embedding_dim),dtype=np.float32)\n",
    "\n",
    "for word,i in x_t.word_index.items():\n",
    "    embedding_vector = embedding_matrix.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_m[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder Decoder Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_layer = Input(shape=(max_text_len,), name=\"encoder_input_layer\")\n",
    "\n",
    "# The Embedding Layer\n",
    "\n",
    "embedding_layer_input = Embedding(input_dim=x_vocab_size, output_dim=300, trainable=False, input_length=max_text_len, weights=[embedding_matrix], name=\"encoder_embedding_layer\")(encoder_input_layer)\n",
    "\n",
    "# print(embedding_layer_input.shape)\n",
    "\n",
    "# encoder\n",
    "\n",
    "encoder_lstm_1,state_h1,state_c1 = LSTM(500, return_sequences= True,return_state=True, name=\"encoder_lstm_1\")(embedding_layer_input)\n",
    "encoder_lstm_2,state_h2,state_c2 = LSTM(500, return_sequences= True,return_state=True, name=\"encoder_lstm_2\")(encoder_lstm_1)\n",
    "encoder_lstm_3,state_h3,state_c3 = LSTM(500, return_sequences= True,return_state=True, name=\"encoder_lstm_3\")(encoder_lstm_2)\n",
    "\n",
    "output_encoder, state_h, state_c = LSTM(500, return_state=True, return_sequences=True, name=\"encoder_output_layer\")(encoder_lstm_3)\n",
    "encoder_states = [state_h,state_c]\n",
    "\n",
    "\n",
    "decoder_input_layer = Input(shape=(None,), name=\"decoder_input_layer\")\n",
    "\n",
    "embedding_layer_output = Embedding(input_dim=y_vocab_size, output_dim=300, trainable=False, input_length=max_text_len, weights=[embedding_matrix], name=\"decoder_embedding_layer\")(decoder_input_layer)\n",
    "\n",
    "# decoder\n",
    "\n",
    "# decoder_lstm_1,_,_ = LSTM(256, return_sequences=True, name=\"decoder_lstm_1\")(embedding_layer_output,initial_state=encoder_states)\n",
    "\n",
    "output_decoder, state_h, state_c = LSTM(500, return_state=True, return_sequences=True, name=\"decoder_output_layer\")(embedding_layer_output,initial_state=encoder_states)\n",
    "\n",
    "## Attention Layer\n",
    "attn_layer = AttentionLayer(name = 'attention_layer')\n",
    "attn_out, attn_states = attn_layer([output_encoder, output_decoder])   # based on the encoder and decoder outputs, determine which nodes are more important\n",
    "\n",
    "# ## Concat attention output and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis = -1, name = 'concat_layer')([output_decoder, attn_out])\n",
    "\n",
    "## Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_vocab_size, activation='softmax'))(decoder_concat_input)\n",
    "\n",
    "# decoder_states = [state_h,state_c]\n",
    "# output_dense = Dense(len(tokens_output), activation='softmax',name=\"output_dense\")(output_decoder)\n",
    "\n",
    "model = Model(inputs=[encoder_input_layer,decoder_input_layer], outputs=[decoder_dense])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  2/149 [..............................] - ETA: 1:05:23 - loss: 10.6062"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39mcheckpoint_filepath,save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpadded_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpaddad_y_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaddad_y_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaddad_y_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpaddad_y_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpaddad_X_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadded_y_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded_y_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_y_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadded_y_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "checkpoint_filepath = './model.{epoch:02d}-{val_loss:.2f}.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True, save_freq = \"epoch\")\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
    "history=model.fit([padded_X_train,paddad_y_train[:,:-1]], paddad_y_train.reshape(paddad_y_train.shape[0],paddad_y_train.shape[1], 1)[:,1:] ,epochs=10,batch_size=512, validation_data=([paddad_X_val,padded_y_val[:,:-1]], padded_y_val.reshape(padded_y_val.shape[0],padded_y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'AttentionLayer': AttentionLayer}\n",
    "with keras.utils.custom_object_scope(custom_objects):\n",
    "    loaded_model = load_model(r\"C:\\Users\\hp\\Downloads\\test_model.h5\")\n",
    "    # loaded_model = load_model(r\"D:\\vs code\\python\\DeepLearning\\Projects\\textSummarizer\\Abstractive\\utils\\summariser_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input_layer (Input  [(None, 66)]                 0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " encoder_embedding_layer (E  (None, 66, 300)              3320460   ['encoder_input_layer[0][0]'] \n",
      " mbedding)                                                0                                       \n",
      "                                                                                                  \n",
      " encoder_lstm_1 (LSTM)       [(None, 66, 500),            1602000   ['encoder_embedding_layer[0][0\n",
      "                              (None, 500),                          ]']                           \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " encoder_lstm_2 (LSTM)       [(None, 66, 500),            2002000   ['encoder_lstm_1[0][0]']      \n",
      "                              (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_input_layer (Input  [(None, None)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " encoder_lstm_3 (LSTM)       [(None, 66, 500),            2002000   ['encoder_lstm_2[0][0]']      \n",
      "                              (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_embedding_layer (E  (None, None, 300)            1297920   ['decoder_input_layer[0][0]'] \n",
      " mbedding)                                                0                                       \n",
      "                                                                                                  \n",
      " encoder_output_layer (LSTM  [(None, 66, 500),            2002000   ['encoder_lstm_3[0][0]']      \n",
      " )                            (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_output_layer (LSTM  [(None, None, 500),          1602000   ['decoder_embedding_layer[0][0\n",
      " )                            (None, 500),                          ]',                           \n",
      "                              (None, 500)]                           'encoder_output_layer[0][1]',\n",
      "                                                                     'encoder_output_layer[0][2]']\n",
      "                                                                                                  \n",
      " attention_layer (Attention  [(None, None, 500),          500500    ['encoder_output_layer[0][0]',\n",
      " Layer)                       (None, None, 66)]                      'decoder_output_layer[0][0]']\n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)  (None, None, 1000)           0         ['decoder_output_layer[0][0]',\n",
      "                                                                     'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_Dense_lay  (None, None, 43264)          4330726   ['concat_layer[0][0]']        \n",
      " er (TimeDistributed)                                     4                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99201564 (378.42 MB)\n",
      "Trainable params: 53017764 (202.25 MB)\n",
      "Non-trainable params: 46183800 (176.18 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(loaded_model,show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer at index 0: encoder_input_layer, Type: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer at index 1: encoder_embedding_layer, Type: <class 'keras.src.layers.core.embedding.Embedding'>\n",
      "Layer at index 2: encoder_lstm_1, Type: <class 'keras.src.layers.rnn.lstm.LSTM'>\n",
      "Layer at index 3: encoder_lstm_2, Type: <class 'keras.src.layers.rnn.lstm.LSTM'>\n",
      "Layer at index 4: decoder_input_layer, Type: <class 'keras.src.engine.input_layer.InputLayer'>\n",
      "Layer at index 5: encoder_lstm_3, Type: <class 'keras.src.layers.rnn.lstm.LSTM'>\n",
      "Layer at index 6: decoder_embedding_layer, Type: <class 'keras.src.layers.core.embedding.Embedding'>\n",
      "Layer at index 7: encoder_output_layer, Type: <class 'keras.src.layers.rnn.lstm.LSTM'>\n",
      "Layer at index 8: decoder_output_layer, Type: <class 'keras.src.layers.rnn.lstm.LSTM'>\n",
      "Layer at index 9: attention_layer, Type: <class 'utils.attention.AttentionLayer'>\n",
      "Layer at index 10: concat_layer, Type: <class 'keras.src.layers.merging.concatenate.Concatenate'>\n",
      "Layer at index 11: time_distributed_Dense_layer, Type: <class 'keras.src.layers.rnn.time_distributed.TimeDistributed'>\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(loaded_model.layers):\n",
    "    print(f\"Layer at index {i}: {layer.name}, Type: {type(layer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_input_layer (Input  [(None, 66)]                 0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " encoder_embedding_layer (E  (None, 66, 300)              3320460   ['encoder_input_layer[0][0]'] \n",
      " mbedding)                                                0                                       \n",
      "                                                                                                  \n",
      " encoder_lstm_1 (LSTM)       [(None, 66, 500),            1602000   ['encoder_embedding_layer[0][0\n",
      "                              (None, 500),                          ]']                           \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " encoder_lstm_2 (LSTM)       [(None, 66, 500),            2002000   ['encoder_lstm_1[0][0]']      \n",
      "                              (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_input_layer (Input  [(None, None)]               0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " encoder_lstm_3 (LSTM)       [(None, 66, 500),            2002000   ['encoder_lstm_2[0][0]']      \n",
      "                              (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_embedding_layer (E  (None, None, 300)            1297920   ['decoder_input_layer[0][0]'] \n",
      " mbedding)                                                0                                       \n",
      "                                                                                                  \n",
      " encoder_output_layer (LSTM  [(None, 66, 500),            2002000   ['encoder_lstm_3[0][0]']      \n",
      " )                            (None, 500),                                                        \n",
      "                              (None, 500)]                                                        \n",
      "                                                                                                  \n",
      " decoder_output_layer (LSTM  [(None, None, 500),          1602000   ['decoder_embedding_layer[0][0\n",
      " )                            (None, 500),                          ]',                           \n",
      "                              (None, 500)]                           'encoder_output_layer[0][1]',\n",
      "                                                                     'encoder_output_layer[0][2]']\n",
      "                                                                                                  \n",
      " attention_layer (Attention  [(None, None, 500),          500500    ['encoder_output_layer[0][0]',\n",
      " Layer)                       (None, None, 66)]                      'decoder_output_layer[0][0]']\n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)  (None, None, 1000)           0         ['decoder_output_layer[0][0]',\n",
      "                                                                     'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_Dense_lay  (None, None, 43264)          4330726   ['concat_layer[0][0]']        \n",
      " er (TimeDistributed)                                     4                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99201564 (378.42 MB)\n",
      "Trainable params: 53017764 (202.25 MB)\n",
      "Non-trainable params: 46183800 (176.18 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_encoder = loaded_model.layers[0].input   # taking input\n",
    "\n",
    "embedding_inp = loaded_model.layers[1](input_encoder)\n",
    "\n",
    "# Encoder\n",
    "lstm_enc_1,h1,c1 = loaded_model.layers[2](embedding_inp) # encoder lstm 1\n",
    "lstm_enc_2,h2,c2 = loaded_model.layers[3](lstm_enc_1) # encoder lstm 2\n",
    "lstm_enc_3,h3,c3 = loaded_model.layers[5](lstm_enc_2) # encoder lstm 3\n",
    "lstm_enc_output,h,c = loaded_model.layers[7](lstm_enc_3) # encoder output layer\n",
    "encoder_states = [h,c]\n",
    "\n",
    "model_enc = Model(input_encoder,[lstm_enc_output,encoder_states])  # final encoder model\n",
    "\n",
    "decoder_initial_state_h = Input(shape=(500,))\n",
    "decoder_initial_state_c = Input(shape=(500,))\n",
    "\n",
    "encoder_out = Input(shape=(max_text_len,500,))\n",
    "\n",
    "decoder_states = [decoder_initial_state_h,decoder_initial_state_c]\n",
    "\n",
    "input_decoder = loaded_model.layers[4].input # taking input\n",
    "\n",
    "embedding_out = loaded_model.layers[6](input_decoder)\n",
    "\n",
    "# Decoder\n",
    "lstm_dec_output,h_d,c_d = loaded_model.layers[8](embedding_out,initial_state=decoder_states) # decoder output layer\n",
    "\n",
    "attention_output,attention_states = loaded_model.layers[9]([encoder_out,lstm_dec_output]) # implementing attention mechanism\n",
    "\n",
    "concatenation_output = loaded_model.layers[10]([lstm_dec_output,attention_output]) # concatenation layer\n",
    "\n",
    "time_distributed_layer = loaded_model.layers[11](concatenation_output) # time distributed layer that provides the predicted word\n",
    "\n",
    "decoder_model = Model([input_decoder] + [encoder_out,decoder_states], [time_distributed_layer] + [h_d,c_d]) # final decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word = y_t.index_word\n",
    "reverse_input_word = x_t.index_word\n",
    "target_word_index = y_t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'end',\n",
       " 2: 'start',\n",
       " 3: 'us',\n",
       " 4: 'india',\n",
       " 5: 'man',\n",
       " 6: 'govt',\n",
       " 7: 'pm',\n",
       " 8: 'delhi',\n",
       " 9: '2',\n",
       " 10: 'trump',\n",
       " 11: 'report',\n",
       " 12: 'bjp',\n",
       " 13: 'indian',\n",
       " 14: 'cm',\n",
       " 15: 'woman',\n",
       " 16: '1',\n",
       " 17: 'new',\n",
       " 18: 'modi',\n",
       " 19: '3',\n",
       " 20: 'crore',\n",
       " 21: '1st',\n",
       " 22: 'people',\n",
       " 23: 'police',\n",
       " 24: 'mumbai',\n",
       " 25: 'years',\n",
       " 26: 'sc',\n",
       " 27: 'reports',\n",
       " 28: 'get',\n",
       " 29: 'killed',\n",
       " 30: '5',\n",
       " 31: 'first',\n",
       " 32: 'lakh',\n",
       " 33: 'world',\n",
       " 34: 'case',\n",
       " 35: 'time',\n",
       " 36: 'video',\n",
       " 37: 'pak',\n",
       " 38: 'film',\n",
       " 39: 'china',\n",
       " 40: 'gets',\n",
       " 41: 'minister',\n",
       " 42: 'says',\n",
       " 43: 'rahul',\n",
       " 44: 'arrested',\n",
       " 45: '4',\n",
       " 46: '10',\n",
       " 47: 'may',\n",
       " 48: 'chief',\n",
       " 49: 'women',\n",
       " 50: 'ceo',\n",
       " 51: 'found',\n",
       " 52: 'day',\n",
       " 53: 'uk',\n",
       " 54: 'google',\n",
       " 55: 'test',\n",
       " 56: 'mp',\n",
       " 57: 'win',\n",
       " 58: 'congress',\n",
       " 59: 'like',\n",
       " 60: 'cr',\n",
       " 61: 'hc',\n",
       " 62: '6',\n",
       " 63: 'make',\n",
       " 64: 'made',\n",
       " 65: 'air',\n",
       " 66: 'kohli',\n",
       " 67: 'team',\n",
       " 68: 'held',\n",
       " 69: 'attack',\n",
       " 70: 'asks',\n",
       " 71: 'court',\n",
       " 72: 'prez',\n",
       " 73: 'study',\n",
       " 74: 'pakistan',\n",
       " 75: 'death',\n",
       " 76: 'yrs',\n",
       " 77: 'shares',\n",
       " 78: 'users',\n",
       " 79: 'kerala',\n",
       " 80: 'korea',\n",
       " 81: 'leader',\n",
       " 82: 'dead',\n",
       " 83: 'post',\n",
       " 84: 'cannot',\n",
       " 85: 'year',\n",
       " 86: 'girl',\n",
       " 87: 'cong',\n",
       " 88: '7',\n",
       " 89: 'car',\n",
       " 90: 'jk',\n",
       " 91: 'days',\n",
       " 92: 'ban',\n",
       " 93: 'facebook',\n",
       " 94: 'bank',\n",
       " 95: 'calls',\n",
       " 96: 'one',\n",
       " 97: 'army',\n",
       " 98: 'accused',\n",
       " 99: 'record',\n",
       " 100: 'pay',\n",
       " 101: 'fake',\n",
       " 102: 'apple',\n",
       " 103: 'twitter',\n",
       " 104: 'rape',\n",
       " 105: 'russia',\n",
       " 106: 'body',\n",
       " 107: 'take',\n",
       " 108: 'students',\n",
       " 109: 'fire',\n",
       " 110: 'due',\n",
       " 111: 'school',\n",
       " 112: 'airport',\n",
       " 113: '100',\n",
       " 114: 'player',\n",
       " 115: '8',\n",
       " 116: 'would',\n",
       " 117: 'claims',\n",
       " 118: 'home',\n",
       " 119: 'open',\n",
       " 120: '2018',\n",
       " 121: 'data',\n",
       " 122: 'pic',\n",
       " 123: 'actor',\n",
       " 124: 'wife',\n",
       " 125: 'match',\n",
       " 126: 'train',\n",
       " 127: 'dies',\n",
       " 128: 'khan',\n",
       " 129: 'work',\n",
       " 130: 'worth',\n",
       " 131: 'life',\n",
       " 132: 'kids',\n",
       " 133: 'slams',\n",
       " 134: 'play',\n",
       " 135: 'startup',\n",
       " 136: 'flight',\n",
       " 137: '20',\n",
       " 138: 'launches',\n",
       " 139: 'gst',\n",
       " 140: 'tweets',\n",
       " 141: 'jail',\n",
       " 142: 'president',\n",
       " 143: 'two',\n",
       " 144: 'hits',\n",
       " 145: 'bihar',\n",
       " 146: 'men',\n",
       " 147: 'sexual',\n",
       " 148: 'wins',\n",
       " 149: 'deal',\n",
       " 150: 'show',\n",
       " 151: 'want',\n",
       " 152: 'house',\n",
       " 153: 'salman',\n",
       " 154: 'un',\n",
       " 155: 'hit',\n",
       " 156: 'give',\n",
       " 157: 'using',\n",
       " 158: 'star',\n",
       " 159: 'ipl',\n",
       " 160: 'row',\n",
       " 161: 'use',\n",
       " 162: 'me',\n",
       " 163: 'injured',\n",
       " 164: 'gandhi',\n",
       " 165: 'centre',\n",
       " 166: '12',\n",
       " 167: 'cricket',\n",
       " 168: 'shows',\n",
       " 169: 'priyanka',\n",
       " 170: 'without',\n",
       " 171: 'polls',\n",
       " 172: 'singh',\n",
       " 173: 'state',\n",
       " 174: 'back',\n",
       " 175: 'away',\n",
       " 176: 'let',\n",
       " 177: 'family',\n",
       " 178: 'amid',\n",
       " 179: 'protest',\n",
       " 180: 'go',\n",
       " 181: 'actress',\n",
       " 182: 'government',\n",
       " 183: 'iran',\n",
       " 184: 'months',\n",
       " 185: 'last',\n",
       " 186: 'used',\n",
       " 187: 'n',\n",
       " 188: 'launch',\n",
       " 189: 'money',\n",
       " 190: '9',\n",
       " 191: 'cbi',\n",
       " 192: 'former',\n",
       " 193: 'big',\n",
       " 194: 'help',\n",
       " 195: '50',\n",
       " 196: 'cup',\n",
       " 197: 'released',\n",
       " 198: 'free',\n",
       " 199: 'fb',\n",
       " 200: 'probe',\n",
       " 201: 'media',\n",
       " 202: 'party',\n",
       " 203: 'could',\n",
       " 204: 'set',\n",
       " 205: 'firm',\n",
       " 206: '15',\n",
       " 207: 'war',\n",
       " 208: 'million',\n",
       " 209: 'saudi',\n",
       " 210: 'ktaka',\n",
       " 211: '2017',\n",
       " 212: 'shah',\n",
       " 213: 'kills',\n",
       " 214: 'never',\n",
       " 215: 'tv',\n",
       " 216: 'it',\n",
       " 217: 'chinese',\n",
       " 218: 'takes',\n",
       " 219: 'app',\n",
       " 220: 'sex',\n",
       " 221: 'user',\n",
       " 222: '30',\n",
       " 223: 'makes',\n",
       " 224: 'bill',\n",
       " 225: 'stop',\n",
       " 226: 'staff',\n",
       " 227: 'cops',\n",
       " 228: 'wedding',\n",
       " 229: 'uber',\n",
       " 230: 'tax',\n",
       " 231: 'security',\n",
       " 232: 'billion',\n",
       " 233: 'denies',\n",
       " 234: 'dhoni',\n",
       " 235: 'aadhaar',\n",
       " 236: 'amazon',\n",
       " 237: 'online',\n",
       " 238: 'harassment',\n",
       " 239: 'bengaluru',\n",
       " 240: 'sachin',\n",
       " 241: 'ram',\n",
       " 242: 'seeks',\n",
       " 243: 'got',\n",
       " 244: 'suicide',\n",
       " 245: 'mla',\n",
       " 246: 'face',\n",
       " 247: 'water',\n",
       " 248: 'asked',\n",
       " 249: 'bans',\n",
       " 250: 'gujarat',\n",
       " 251: 'national',\n",
       " 252: 'since',\n",
       " 253: '11',\n",
       " 254: 'plans',\n",
       " 255: 'andhra',\n",
       " 256: 'plane',\n",
       " 257: 'karnataka',\n",
       " 258: 'named',\n",
       " 259: 'space',\n",
       " 260: 'srk',\n",
       " 261: 'head',\n",
       " 262: 'top',\n",
       " 263: 'baby',\n",
       " 264: '2019',\n",
       " 265: 'student',\n",
       " 266: 'runs',\n",
       " 267: 'told',\n",
       " 268: 'aus',\n",
       " 269: 'maha',\n",
       " 270: 'food',\n",
       " 271: 'hospital',\n",
       " 272: 'captain',\n",
       " 273: 'russian',\n",
       " 274: 'missing',\n",
       " 275: 'australia',\n",
       " 276: 'musk',\n",
       " 277: 'north',\n",
       " 278: 'gives',\n",
       " 279: 'gold',\n",
       " 280: 'south',\n",
       " 281: 'love',\n",
       " 282: 'nasa',\n",
       " 283: 'indians',\n",
       " 284: 'law',\n",
       " 285: 'maharashtra',\n",
       " 286: 'becomes',\n",
       " 287: 'become',\n",
       " 288: 'best',\n",
       " 289: 'boy',\n",
       " 290: 'must',\n",
       " 291: 'class',\n",
       " 292: 'news',\n",
       " 293: 'punjab',\n",
       " 294: 'game',\n",
       " 295: 'ranveer',\n",
       " 296: 'board',\n",
       " 297: 'fight',\n",
       " 298: 'goes',\n",
       " 299: 'mn',\n",
       " 300: 'rbi',\n",
       " 301: 'passes',\n",
       " 302: 'bus',\n",
       " 303: 'whatsapp',\n",
       " 304: 'sale',\n",
       " 305: 'times',\n",
       " 306: 'director',\n",
       " 307: 'shot',\n",
       " 308: 'wc',\n",
       " 309: 'need',\n",
       " 310: 'isis',\n",
       " 311: 'run',\n",
       " 312: 'scientists',\n",
       " 313: 'coach',\n",
       " 314: 'son',\n",
       " 315: 'couple',\n",
       " 316: 'odi',\n",
       " 317: 'starrer',\n",
       " 318: 'deepika',\n",
       " 319: 'child',\n",
       " 320: 'good',\n",
       " 321: 'public',\n",
       " 322: 'sets',\n",
       " 323: 'tesla',\n",
       " 324: 'films',\n",
       " 325: 'workers',\n",
       " 326: 'raises',\n",
       " 327: 'metro',\n",
       " 328: 'high',\n",
       " 329: 'orders',\n",
       " 330: 'call',\n",
       " 331: 'next',\n",
       " 332: 'akshay',\n",
       " 333: 'release',\n",
       " 334: 'offers',\n",
       " 335: 'series',\n",
       " 336: 'announces',\n",
       " 337: 'daughter',\n",
       " 338: 'power',\n",
       " 339: 'black',\n",
       " 340: 'name',\n",
       " 341: 'driver',\n",
       " 342: 'tn',\n",
       " 343: 'cop',\n",
       " 344: 'human',\n",
       " 345: 'temple',\n",
       " 346: 'song',\n",
       " 347: 'aap',\n",
       " 348: 'virat',\n",
       " 349: 'posts',\n",
       " 350: 'falls',\n",
       " 351: 'near',\n",
       " 352: 'look',\n",
       " 353: '25',\n",
       " 354: 'strike',\n",
       " 355: 'judge',\n",
       " 356: 'road',\n",
       " 357: 'haryana',\n",
       " 358: 'jaitley',\n",
       " 359: 'reveals',\n",
       " 360: 'goa',\n",
       " 361: 'cases',\n",
       " 362: 'working',\n",
       " 363: 'called',\n",
       " 364: 'kangana',\n",
       " 365: 'list',\n",
       " 366: 'group',\n",
       " 367: 'french',\n",
       " 368: 'hotel',\n",
       " 369: 'girls',\n",
       " 370: 'japan',\n",
       " 371: 'issues',\n",
       " 372: 'breaks',\n",
       " 373: 'city',\n",
       " 374: 'files',\n",
       " 375: 'notice',\n",
       " 376: 'leave',\n",
       " 377: 'change',\n",
       " 378: 'buy',\n",
       " 379: 'visit',\n",
       " 380: 'fans',\n",
       " 381: 'bn',\n",
       " 382: 'employees',\n",
       " 383: 'phone',\n",
       " 384: 'schools',\n",
       " 385: 'kapoor',\n",
       " 386: 'players',\n",
       " 387: 'officer',\n",
       " 388: 'cut',\n",
       " 389: 'terror',\n",
       " 390: 'beat',\n",
       " 391: '2nd',\n",
       " 392: 'firms',\n",
       " 393: 'pradesh',\n",
       " 394: 'return',\n",
       " 395: 'farmers',\n",
       " 396: 'meet',\n",
       " 397: 'kill',\n",
       " 398: 'talks',\n",
       " 399: 'final',\n",
       " 400: 'rajasthan',\n",
       " 401: 'kumar',\n",
       " 402: 'hours',\n",
       " 403: 'sena',\n",
       " 404: 'union',\n",
       " 405: 'marriage',\n",
       " 406: 'award',\n",
       " 407: '40',\n",
       " 408: 'jailed',\n",
       " 409: 'station',\n",
       " 410: 'cars',\n",
       " 411: 'mother',\n",
       " 412: 'fan',\n",
       " 413: 'kejriwal',\n",
       " 414: 'murder',\n",
       " 415: 'feature',\n",
       " 416: 'railways',\n",
       " 417: 'muslim',\n",
       " 418: '18',\n",
       " 419: 'every',\n",
       " 420: 'husband',\n",
       " 421: 'lost',\n",
       " 422: 'kashmir',\n",
       " 423: 'issue',\n",
       " 424: 'live',\n",
       " 425: 'female',\n",
       " 426: 'ahead',\n",
       " 427: 'support',\n",
       " 428: 'part',\n",
       " 429: 'trailer',\n",
       " 430: 'company',\n",
       " 431: 'sri',\n",
       " 432: '13',\n",
       " 433: 'ever',\n",
       " 434: 'plan',\n",
       " 435: 'wrong',\n",
       " 436: 'tests',\n",
       " 437: 'father',\n",
       " 438: 'booked',\n",
       " 439: 'official',\n",
       " 440: 'ranbir',\n",
       " 441: 'real',\n",
       " 442: 'apologises',\n",
       " 443: 'nuclear',\n",
       " 444: 'viral',\n",
       " 445: 'see',\n",
       " 446: 'remark',\n",
       " 447: 'tech',\n",
       " 448: 'football',\n",
       " 449: 'attacks',\n",
       " 450: '16',\n",
       " 451: 'country',\n",
       " 452: 'raj',\n",
       " 453: 'london',\n",
       " 454: 'shut',\n",
       " 455: 'kapil',\n",
       " 456: 'pics',\n",
       " 457: 'military',\n",
       " 458: 'given',\n",
       " 459: 'loses',\n",
       " 460: 'fined',\n",
       " 461: 'jobs',\n",
       " 462: 'flipkart',\n",
       " 463: 'foreign',\n",
       " 464: 'states',\n",
       " 465: 'protests',\n",
       " 466: 'ai',\n",
       " 467: 'ago',\n",
       " 468: 'b',\n",
       " 469: 'jet',\n",
       " 470: 'cancer',\n",
       " 471: 'fraud',\n",
       " 472: 'right',\n",
       " 473: 'bcci',\n",
       " 474: 'iphone',\n",
       " 475: 'banks',\n",
       " 476: 'sell',\n",
       " 477: 'tells',\n",
       " 478: 'syria',\n",
       " 479: 'white',\n",
       " 480: 'event',\n",
       " 481: 'birthday',\n",
       " 482: 'paid',\n",
       " 483: 'bomb',\n",
       " 484: 'order',\n",
       " 485: 'tweet',\n",
       " 486: 'suspended',\n",
       " 487: 'making',\n",
       " 488: 'rss',\n",
       " 489: 'violence',\n",
       " 490: 'chairman',\n",
       " 491: 'tamil',\n",
       " 492: 'illegal',\n",
       " 493: 'alia',\n",
       " 494: 'putin',\n",
       " 495: 'slammed',\n",
       " 496: 'even',\n",
       " 497: 'rules',\n",
       " 498: 'find',\n",
       " 499: 'england',\n",
       " 500: 'system',\n",
       " 501: 'rohit',\n",
       " 502: 'bluru',\n",
       " 503: 'ronaldo',\n",
       " 504: 'lose',\n",
       " 505: 'come',\n",
       " 506: 'service',\n",
       " 507: 'border',\n",
       " 508: 'banned',\n",
       " 509: 'warns',\n",
       " 510: 'children',\n",
       " 511: 'meeting',\n",
       " 512: 'die',\n",
       " 513: 'assembly',\n",
       " 514: 'theatres',\n",
       " 515: 'quits',\n",
       " 516: 'social',\n",
       " 517: 'till',\n",
       " 518: 'officials',\n",
       " 519: 'bitcoin',\n",
       " 520: 'victims',\n",
       " 521: 'watch',\n",
       " 522: 'threat',\n",
       " 523: 'model',\n",
       " 524: 'refuses',\n",
       " 525: 'sharma',\n",
       " 526: 'railway',\n",
       " 527: 'better',\n",
       " 528: 'no',\n",
       " 529: 'panel',\n",
       " 530: 'afghanistan',\n",
       " 531: 'instagram',\n",
       " 532: 'scam',\n",
       " 533: 'fund',\n",
       " 534: 'know',\n",
       " 535: '14',\n",
       " 536: 'job',\n",
       " 537: 'parliament',\n",
       " 538: 'crash',\n",
       " 539: 'said',\n",
       " 540: 'loss',\n",
       " 541: 'debut',\n",
       " 542: 'elections',\n",
       " 543: 'number',\n",
       " 544: 'building',\n",
       " 545: 'mlas',\n",
       " 546: 'anushka',\n",
       " 547: 'canada',\n",
       " 548: 'behind',\n",
       " 549: 'fir',\n",
       " 550: 'office',\n",
       " 551: 'late',\n",
       " 552: 'allows',\n",
       " 553: 'parents',\n",
       " 554: 'inside',\n",
       " 555: 'claim',\n",
       " 556: 'getting',\n",
       " 557: 'france',\n",
       " 558: 'ball',\n",
       " 559: 'action',\n",
       " 560: 'global',\n",
       " 561: 'rejects',\n",
       " 562: 'aamir',\n",
       " 563: '3rd',\n",
       " 564: 'exam',\n",
       " 565: 'arrest',\n",
       " 566: 'oil',\n",
       " 567: 'taking',\n",
       " 568: 'sabha',\n",
       " 569: '500',\n",
       " 570: 'build',\n",
       " 571: 'triple',\n",
       " 572: 'donald',\n",
       " 573: 'still',\n",
       " 574: 'terrorists',\n",
       " 575: 'drunk',\n",
       " 576: 'special',\n",
       " 577: 'despite',\n",
       " 578: '200',\n",
       " 579: 'way',\n",
       " 580: 'accident',\n",
       " 581: 'biggest',\n",
       " 582: 'old',\n",
       " 583: 'election',\n",
       " 584: '17',\n",
       " 585: 'infosys',\n",
       " 586: 'leaders',\n",
       " 587: 'intl',\n",
       " 588: 'move',\n",
       " 589: 'three',\n",
       " 590: 'minor',\n",
       " 591: 'trying',\n",
       " 592: 'vs',\n",
       " 593: '24',\n",
       " 594: 'second',\n",
       " 595: 'victim',\n",
       " 596: 'teen',\n",
       " 597: 'trolls',\n",
       " 598: 'raped',\n",
       " 599: 'odisha',\n",
       " 600: '1000',\n",
       " 601: 'kg',\n",
       " 602: 'west',\n",
       " 603: 'sanctions',\n",
       " 604: 'together',\n",
       " 605: 'israel',\n",
       " 606: 'hold',\n",
       " 607: 'sonam',\n",
       " 608: 'assam',\n",
       " 609: 'approves',\n",
       " 610: 'birth',\n",
       " 611: 'among',\n",
       " 612: 'yogi',\n",
       " 613: 'australian',\n",
       " 614: 'airline',\n",
       " 615: 'join',\n",
       " 616: 'karan',\n",
       " 617: 'seized',\n",
       " 618: 'wanted',\n",
       " 619: 'unveils',\n",
       " 620: 'trade',\n",
       " 621: 'role',\n",
       " 622: 'tiger',\n",
       " 623: 'business',\n",
       " 624: 'forced',\n",
       " 625: 'hindu',\n",
       " 626: 'cash',\n",
       " 627: 'act',\n",
       " 628: 'boss',\n",
       " 629: 'uses',\n",
       " 630: 'land',\n",
       " 631: 'defence',\n",
       " 632: 'playing',\n",
       " 633: 'youth',\n",
       " 634: 'left',\n",
       " 635: 'abuse',\n",
       " 636: 'say',\n",
       " 637: 'swaraj',\n",
       " 638: 'ali',\n",
       " 639: 'took',\n",
       " 640: 'raping',\n",
       " 641: 'bengal',\n",
       " 642: 'university',\n",
       " 643: 'quit',\n",
       " 644: 'drug',\n",
       " 645: 'paris',\n",
       " 646: 'fine',\n",
       " 647: 'sold',\n",
       " 648: 'sues',\n",
       " 649: 'race',\n",
       " 650: 'elon',\n",
       " 651: 'week',\n",
       " 652: 'min',\n",
       " 653: 'johar',\n",
       " 654: 'private',\n",
       " 655: 'charges',\n",
       " 656: 'poll',\n",
       " 657: 'sends',\n",
       " 658: 'threatens',\n",
       " 659: 'mobile',\n",
       " 660: 'hai',\n",
       " 661: 'faces',\n",
       " 662: 'returns',\n",
       " 663: 'allow',\n",
       " 664: 'miss',\n",
       " 665: 'ready',\n",
       " 666: 'offer',\n",
       " 667: 'amit',\n",
       " 668: 'married',\n",
       " 669: 'caught',\n",
       " 670: 'secy',\n",
       " 671: 'nirav',\n",
       " 672: 'talaq',\n",
       " 673: 'save',\n",
       " 674: 'history',\n",
       " 675: 'bday',\n",
       " 676: 'selling',\n",
       " 677: 'german',\n",
       " 678: 'accuses',\n",
       " 679: 'awards',\n",
       " 680: 'another',\n",
       " 681: 'forces',\n",
       " 682: 'demands',\n",
       " 683: 'rafale',\n",
       " 684: 'releases',\n",
       " 685: 'tata',\n",
       " 686: 'germany',\n",
       " 687: 'microsoft',\n",
       " 688: 'rights',\n",
       " 689: 'opens',\n",
       " 690: 'put',\n",
       " 691: 'earth',\n",
       " 692: 'share',\n",
       " 693: 'names',\n",
       " 694: 'traffic',\n",
       " 695: 'note',\n",
       " 696: 'founder',\n",
       " 697: 'health',\n",
       " 698: 'up',\n",
       " 699: 'games',\n",
       " 700: 'force',\n",
       " 701: 'plea',\n",
       " 702: 'dog',\n",
       " 703: 'assault',\n",
       " 704: 'think',\n",
       " 705: 'airways',\n",
       " 706: 'wants',\n",
       " 707: 'policy',\n",
       " 708: 'reach',\n",
       " 709: 'bharat',\n",
       " 710: 'poster',\n",
       " 711: 'katrina',\n",
       " 712: 'fifa',\n",
       " 713: 'gurugram',\n",
       " 714: 'cow',\n",
       " 715: 'emergency',\n",
       " 716: 'ad',\n",
       " 717: 'mallya',\n",
       " 718: 'floods',\n",
       " 719: 'ec',\n",
       " 720: 'scores',\n",
       " 721: 'bad',\n",
       " 722: 'wb',\n",
       " 723: 'muslims',\n",
       " 724: 'liquor',\n",
       " 725: 'date',\n",
       " 726: 'passengers',\n",
       " 727: 'political',\n",
       " 728: 'rescued',\n",
       " 729: 'much',\n",
       " 730: '300',\n",
       " 731: 'sent',\n",
       " 732: 'travel',\n",
       " 733: 'photo',\n",
       " 734: '21',\n",
       " 735: 'campaign',\n",
       " 736: 'stake',\n",
       " 737: 'book',\n",
       " 738: 'sehwag',\n",
       " 739: 'outside',\n",
       " 740: 'term',\n",
       " 741: 'singer',\n",
       " 742: 'mamata',\n",
       " 743: 'shahid',\n",
       " 744: 'killing',\n",
       " 745: 'cost',\n",
       " 746: 'loan',\n",
       " 747: 'records',\n",
       " 748: 'telangana',\n",
       " 749: 'crashes',\n",
       " 750: 'title',\n",
       " 751: 'card',\n",
       " 752: 'college',\n",
       " 753: 'picture',\n",
       " 754: 'bail',\n",
       " 755: 'cuts',\n",
       " 756: 'person',\n",
       " 757: 'you',\n",
       " 758: 'club',\n",
       " 759: 'lead',\n",
       " 760: 'yadav',\n",
       " 761: 'united',\n",
       " 762: 'stay',\n",
       " 763: 'korean',\n",
       " 764: 'hyderabad',\n",
       " 765: 'ed',\n",
       " 766: 'price',\n",
       " 767: 'doctors',\n",
       " 768: 'teacher',\n",
       " 769: 'mps',\n",
       " 770: 'break',\n",
       " 771: 'finds',\n",
       " 772: '2016',\n",
       " 773: 'seats',\n",
       " 774: 'challenge',\n",
       " 775: 'varun',\n",
       " 776: 'pilot',\n",
       " 777: 'trolled',\n",
       " 778: 'today',\n",
       " 779: 'great',\n",
       " 780: 'arjun',\n",
       " 781: 'militants',\n",
       " 782: 'month',\n",
       " 783: 'remove',\n",
       " 784: 'rises',\n",
       " 785: 'march',\n",
       " 786: 'soldiers',\n",
       " 787: 'urges',\n",
       " 788: 'noida',\n",
       " 789: '22',\n",
       " 790: 'met',\n",
       " 791: 'red',\n",
       " 792: 'per',\n",
       " 793: 'safety',\n",
       " 794: 'kolkata',\n",
       " 795: 'doctor',\n",
       " 796: 'died',\n",
       " 797: 'happy',\n",
       " 798: 'host',\n",
       " 799: 'politics',\n",
       " 800: 'proposes',\n",
       " 801: 'send',\n",
       " 802: 'eu',\n",
       " 803: 'hike',\n",
       " 804: 'aged',\n",
       " 805: 'medical',\n",
       " 806: 'bwood',\n",
       " 807: 'employee',\n",
       " 808: 'letter',\n",
       " 809: 'toilet',\n",
       " 810: 'hindi',\n",
       " 811: 'age',\n",
       " 812: 'score',\n",
       " 813: 'single',\n",
       " 814: 'internet',\n",
       " 815: 'stolen',\n",
       " 816: 'rajnath',\n",
       " 817: 'icc',\n",
       " 818: 'charge',\n",
       " 819: 'via',\n",
       " 820: 'create',\n",
       " 821: 'nobel',\n",
       " 822: 'electric',\n",
       " 823: 'complaint',\n",
       " 824: 'beats',\n",
       " 825: 'calling',\n",
       " 826: 'going',\n",
       " 827: '60',\n",
       " 828: 'seen',\n",
       " 829: 'gave',\n",
       " 830: 'speech',\n",
       " 831: 'notes',\n",
       " 832: 'kareena',\n",
       " 833: 'paper',\n",
       " 834: 'attend',\n",
       " 835: '19',\n",
       " 836: 'anil',\n",
       " 837: 'ministry',\n",
       " 838: 'services',\n",
       " 839: 'highest',\n",
       " 840: 'goal',\n",
       " 841: 'paytm',\n",
       " 842: 'alliance',\n",
       " 843: 'mark',\n",
       " 844: 'toll',\n",
       " 845: 'pictures',\n",
       " 846: 'moon',\n",
       " 847: 'supreme',\n",
       " 848: 'filed',\n",
       " 849: 'shooting',\n",
       " 850: 'british',\n",
       " 851: 'girlfriend',\n",
       " 852: 'maker',\n",
       " 853: 'driving',\n",
       " 854: 'starts',\n",
       " 855: 'flights',\n",
       " 856: 'plastic',\n",
       " 857: 'leak',\n",
       " 858: 'around',\n",
       " 859: 'less',\n",
       " 860: 'store',\n",
       " 861: 'keep',\n",
       " 862: 'wearing',\n",
       " 863: 'ms',\n",
       " 864: 'fall',\n",
       " 865: 'blast',\n",
       " 866: 'shiv',\n",
       " 867: 'resigns',\n",
       " 868: 'self',\n",
       " 869: 'boys',\n",
       " 870: 'cover',\n",
       " 871: 'contest',\n",
       " 872: 'nothing',\n",
       " 873: 'nadu',\n",
       " 874: 'sports',\n",
       " 875: 'form',\n",
       " 876: 'avoid',\n",
       " 877: 'dad',\n",
       " 878: 'failed',\n",
       " 879: 'phones',\n",
       " 880: 'missile',\n",
       " 881: 'music',\n",
       " 882: 'trophy',\n",
       " 883: 'kim',\n",
       " 884: 'smith',\n",
       " 885: 'hacked',\n",
       " 886: 'helps',\n",
       " 887: 'track',\n",
       " 888: 'sea',\n",
       " 889: 'giving',\n",
       " 890: 'ask',\n",
       " 891: 'career',\n",
       " 892: 'anniversary',\n",
       " 893: 'sabarimala',\n",
       " 894: 'vote',\n",
       " 895: 'village',\n",
       " 896: 'civic',\n",
       " 897: 'metoo',\n",
       " 898: 'dance',\n",
       " 899: 'ends',\n",
       " 900: 'friend',\n",
       " 901: '2000',\n",
       " 902: 'trains',\n",
       " 903: 'mars',\n",
       " 904: 'dating',\n",
       " 905: 'truck',\n",
       " 906: 'lanka',\n",
       " 907: 'ganguly',\n",
       " 908: 'lalu',\n",
       " 909: 'federer',\n",
       " 910: 'denied',\n",
       " 911: 'blood',\n",
       " 912: 'himachal',\n",
       " 913: 't20i',\n",
       " 914: 'cbse',\n",
       " 915: 'sl',\n",
       " 916: 'demand',\n",
       " 917: 'hardik',\n",
       " 918: 'afghan',\n",
       " 919: 'catch',\n",
       " 920: 'now',\n",
       " 921: 'gay',\n",
       " 922: 'place',\n",
       " 923: 'guj',\n",
       " 924: 'sign',\n",
       " 925: 'wear',\n",
       " 926: 'fashion',\n",
       " 927: 'tharoor',\n",
       " 928: 'fuel',\n",
       " 929: 'verdict',\n",
       " 930: 'accounts',\n",
       " 931: 'site',\n",
       " 932: 'solar',\n",
       " 933: 'account',\n",
       " 934: 'within',\n",
       " 935: 'chhattisgarh',\n",
       " 936: 'ads',\n",
       " 937: 'mom',\n",
       " 938: 'pnb',\n",
       " 939: 'fired',\n",
       " 940: '23',\n",
       " 941: 'enter',\n",
       " 942: 'stuck',\n",
       " 943: 'officers',\n",
       " 944: 'sanjay',\n",
       " 945: 'biopic',\n",
       " 946: 'straight',\n",
       " 947: 'control',\n",
       " 948: 'pays',\n",
       " 949: 'gun',\n",
       " 950: 'confirms',\n",
       " 951: 'pollution',\n",
       " 952: 'vijay',\n",
       " 953: 'across',\n",
       " 954: 'search',\n",
       " 955: 'needs',\n",
       " 956: 'rishi',\n",
       " 957: 'led',\n",
       " 958: 'funding',\n",
       " 959: 'rumours',\n",
       " 960: 'seeking',\n",
       " 961: 'admits',\n",
       " 962: '31',\n",
       " 963: 'four',\n",
       " 964: 'fm',\n",
       " 965: 'android',\n",
       " 966: 'cofounder',\n",
       " 967: 'feel',\n",
       " 968: 'raise',\n",
       " 969: 'reliance',\n",
       " 970: 'clean',\n",
       " 971: '70',\n",
       " 972: 'youtube',\n",
       " 973: 'nations',\n",
       " 974: 'league',\n",
       " 975: 'night',\n",
       " 976: 'others',\n",
       " 977: 'penalty',\n",
       " 978: 'teachers',\n",
       " 979: 'visa',\n",
       " 980: 'blue',\n",
       " 981: 'prize',\n",
       " 982: 'international',\n",
       " 983: 'padmavati',\n",
       " 984: 'sbi',\n",
       " 985: 'corruption',\n",
       " 986: 'peace',\n",
       " 987: 'wishes',\n",
       " 988: 'statue',\n",
       " 989: 'actors',\n",
       " 990: 'jio',\n",
       " 991: 'indigo',\n",
       " 992: 'justice',\n",
       " 993: 'defeat',\n",
       " 994: 'plant',\n",
       " 995: 'turns',\n",
       " 996: 'dept',\n",
       " 997: 'pandya',\n",
       " 998: 'him',\n",
       " 999: 'bridge',\n",
       " 1000: 'talk',\n",
       " ...}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise(sentence):\n",
    "    sentence= sentence.reshape(1,max_text_len)\n",
    "    encoder_out,encoder_states = model_enc(sentence)\n",
    "    \n",
    "    target_sentence = ''\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0,0] = target_word_index['start']\n",
    "    stop_condition = False\n",
    "    \n",
    "    enc_out,states_enc = model_enc.predict(sentence)\n",
    "    \n",
    "    decoder_stat = states_enc\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        word,s_h,s_c = decoder_model.predict([target_seq] + [enc_out,decoder_stat])\n",
    "        \n",
    "        token = np.argmax(word[0,-1,:])\n",
    "        \n",
    "        target_word = reverse_target_word[token]\n",
    "        \n",
    "        if token != 1:\n",
    "            target_sentence = target_sentence + target_word + ' '\n",
    "            \n",
    "        if token == 1 or len(target_sentence.split()) >= (max_headline_len) - 1:\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0,0] = token\n",
    "        decoder_stat = [s_h,s_c]\n",
    "        \n",
    "    return target_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
    "        newString=newString+reverse_target_word[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+reverse_input_word[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: iranislamic revolutionary guard corps sunday declared victory antigovernment protests blamed enemies irgc blamed countries including us israel saudi arabia unrest many 22 people killed 1000 arrested protests began december according iranian officials \n",
      "Original summary: iran declares victory antigovernment protests \n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 578ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted summary: isis leaderdeath us us jerusalem iraq \n",
      "\n",
      "\n",
      "Review: serving 14 months ceo paytm payments bank renu satti resigned serve coo paytmnew initiative new retail paytm reportedly talks usbased investors existing shareholder softbank raise 500 million same new retail coined jack ma founder alibaba also backs paytm \n",
      "Original summary: paytm payments bank ceo resigns lead new retail \n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Predicted summary: flipkart appoints mohit agarwal quits facebook \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview:\u001b[39m\u001b[38;5;124m\"\u001b[39m,seq2text(\u001b[43mpadded_X_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m))\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,seq2summary(paddad_y_test[i]))\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m,summarise(padded_X_test[i]))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "  print(\"Review:\",seq2text(padded_X_test[i]))\n",
    "  print(\"Original summary:\",seq2summary(paddad_y_test[i]))\n",
    "  print(\"Predicted summary:\",summarise(padded_X_test[i]))\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22948,  8600,  2073,  8247,   108,   706,  1216,  8216,   747,\n",
       "         1842,  7763, 28893,  1842,   332,    97,     5,  1267,   478,\n",
       "         1374,  6553,   210,   781,     8,    70,   820,    62,   747,\n",
       "         1152,   408,    66,  1804,    84,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0],\n",
       "       [ 1636,   543,   171,   128,  1739,  1115,   121, 18490, 24438,\n",
       "         1467,  1881,  4262, 69460,  1778,    20,  2328,  1739,    28,\n",
       "          708,  1049,  1275,  2008,  5815,  2298,  1235,   725,    93,\n",
       "         2357,    20,  2328, 11910,  2331, 14418,   932,  3558,     7,\n",
       "         6371,  1739,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "0.05025188014078477\n"
     ]
    }
   ],
   "source": [
    "def BLEU_Score(y_test, y_pred):\n",
    "    references = [[seq2summary(y_test).split(\" \")]]\n",
    "    candidates = [summarise(y_pred.reshape(1, max_text_len)).split(\" \")]\n",
    "\n",
    "    # Apply smoothing function\n",
    "    smooth = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth)\n",
    "    \n",
    "    return bleu_score\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "scores=[]\n",
    "for i in range(0,2):\n",
    "    scores.append(BLEU_Score(paddad_y_test[i],padded_X_test[i]))\n",
    "    \n",
    "print(np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
